{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# weights & bias for nn layers\n",
    "with tf.variable_scope('layer1') as scope:\n",
    "    W1 = tf.get_variable(\"W\", shape=[784, 512],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([512]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "    \n",
    "    tf.summary.histogram('X',X)\n",
    "    tf.summary.histogram('w1',W1)\n",
    "    tf.summary.histogram('b1',b1)\n",
    "    tf.summary.histogram('layer1',L1)\n",
    "\n",
    "with tf.variable_scope('layer2') as scope:\n",
    "    W2 = tf.get_variable(\"W\", shape=[512, 512],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([512]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "    tf.summary.histogram('w2',W2)\n",
    "    tf.summary.histogram('b2',b2)\n",
    "    tf.summary.histogram('layer2',L2)\n",
    "    \n",
    "with tf.variable_scope('layer3') as scope:\n",
    "    W3 = tf.get_variable(\"W\", shape=[512, 512],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([512]))\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "    tf.summary.histogram('w2',W2)\n",
    "    tf.summary.histogram('b2',b2)\n",
    "    tf.summary.histogram('layer2',L2)\n",
    "    \n",
    "with tf.variable_scope('layer4') as scope:\n",
    "    W4 = tf.get_variable(\"W\", shape=[512, 512],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([512]))\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "    tf.summary.histogram('w2',W2)\n",
    "    tf.summary.histogram('b2',b2)\n",
    "    tf.summary.histogram('layer2',L2)\n",
    "with tf.variable_scope('layer5') as scope:\n",
    "    W5 = tf.get_variable(\"W\", shape=[512, 10],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([10]))\n",
    "    hypothesis = tf.matmul(L4, W5) + b5\n",
    "    \n",
    "    tf.summary.histogram('w5',W5)\n",
    "    tf.summary.histogram('b5',b5)\n",
    "    tf.summary.histogram('logits',hypothesis)\n",
    "\n",
    "print('w1 , w5 = ',W1,',', W5)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "# More name scopes will clean up graph representation\n",
    "with tf.name_scope(\"loss_function\") as scope:\n",
    "    # Minimize error using cross entropy\n",
    "    # Cross entropy\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    # Create a summary to monitor the cost function\n",
    "    tf.summary.scalar(\"loss_function\", loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "merged_summary = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "w1 , w5 =  <tf.Variable 'layer1/W:0' shape=(784, 512) dtype=float32_ref> , <tf.Variable 'layer5/W:0' shape=(512, 10) dtype=float32_ref>\n",
      "Epoch: 0001 cost = 0.437209\n",
      "Epoch: 0002 cost = 0.159190\n",
      "Epoch: 0003 cost = 0.120703\n",
      "Epoch: 0004 cost = 0.099932\n",
      "Epoch: 0005 cost = 0.085817\n",
      "Epoch: 0006 cost = 0.077048\n",
      "Epoch: 0007 cost = 0.067017\n",
      "Epoch: 0008 cost = 0.063526\n",
      "Epoch: 0009 cost = 0.056375\n",
      "Epoch: 0010 cost = 0.051378\n",
      "Epoch: 0011 cost = 0.052023\n",
      "Epoch: 0012 cost = 0.044956\n",
      "Epoch: 0013 cost = 0.043594\n",
      "Epoch: 0014 cost = 0.043045\n",
      "Epoch: 0015 cost = 0.041593\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#tensorboard\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "summary_writer=tf.summary.FileWriter('/graphs',graph_def=sess.graph_def)\n",
    "\n",
    "# train my model\n",
    "global_step =0\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        s,l,_ =sess.run([summary,loss,optimizer], feed_dict=feed_dict)\n",
    "        loss_summ = tf.summary.scalar('loss',loss)\n",
    "        summary_str = sess.run(merged_summary, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        summary_writer.add_summary(summary_str, epoch * total_batch + i)\n",
    "        global_step +=1\n",
    "\n",
    "        avg_cost += sess.run(loss, feed_dict=feed_dict)/ total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "'''\n",
    "# Add summary scalar node\n",
    "acc_summ = tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "\n",
    "#Collect all summaries\n",
    "merged_summary = tf.summary.merge([acc_summ, loss_summ])]\n",
    "\n",
    "summ_writer = tf.summary.FileWriter(dir_path)\n",
    "\n",
    "# session run (To be put inside batch-loop to log loss for each batch)\n",
    "summary = session.run([merged_summary], feed_dict='your input')\n",
    "summ_writer.add_summary(summary, global_step)\n",
    "\n",
    "# Finally close the writer\n",
    "summ_writer.close()\n",
    "writer.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EX2~2\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "CHECK_POINT_DIR = TB_SUMMARY_DIR = './tb/mnist3'\n",
    "\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Image input\n",
    "x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "tf.summary.image('input', x_image, 3)\n",
    "\n",
    "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "with tf.variable_scope('layer1'):\n",
    "    W1 = tf.get_variable(\"W\", shape=[784, 512],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([512]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "    tf.summary.histogram(\"X\", X)\n",
    "    tf.summary.histogram(\"weights\", W1)\n",
    "    tf.summary.histogram(\"bias\", b1)\n",
    "    tf.summary.histogram(\"layer\", L1)\n",
    "\n",
    "with tf.variable_scope('layer2'):\n",
    "    W2 = tf.get_variable(\"W\", shape=[512, 512],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([512]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "    tf.summary.histogram(\"weights\", W2)\n",
    "    tf.summary.histogram(\"bias\", b2)\n",
    "    tf.summary.histogram(\"layer\", L2)\n",
    "\n",
    "with tf.variable_scope('layer3'):\n",
    "    W3 = tf.get_variable(\"W\", shape=[512, 512],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([512]))\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "    tf.summary.histogram(\"weights\", W3)\n",
    "    tf.summary.histogram(\"bias\", b3)\n",
    "    tf.summary.histogram(\"layer\", L3)\n",
    "\n",
    "with tf.variable_scope('layer4'):\n",
    "    W4 = tf.get_variable(\"W\", shape=[512, 512],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([512]))\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "    tf.summary.histogram(\"weights\", W4)\n",
    "    tf.summary.histogram(\"bias\", b4)\n",
    "    tf.summary.histogram(\"layer\", L4)\n",
    "\n",
    "with tf.variable_scope('layer5'):\n",
    "    W5 = tf.get_variable(\"W\", shape=[512, 10],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([10]))\n",
    "    hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "    tf.summary.histogram(\"weights\", W5)\n",
    "    tf.summary.histogram(\"bias\", b5)\n",
    "    tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "\n",
    "last_epoch = tf.Variable(0, name='last_epoch')\n",
    "\n",
    "# Summary\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Create summary writer\n",
    "writer = tf.summary.FileWriter(TB_SUMMARY_DIR)\n",
    "writer.add_graph(sess.graph)\n",
    "global_step = 0\n",
    "\n",
    "# Saver and Restore\n",
    "saver = tf.train.Saver()\n",
    "checkpoint = tf.train.get_checkpoint_state(CHECK_POINT_DIR)\n",
    "\n",
    "if checkpoint and checkpoint.model_checkpoint_path:\n",
    "    try:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    except:\n",
    "        print(\"Error on loading old network weights\")\n",
    "else:\n",
    "    print(\"Could not find old network weights\")\n",
    "\n",
    "start_from = sess.run(last_epoch)\n",
    "\n",
    "# train my model\n",
    "print('Start learning from:', start_from)\n",
    "\n",
    "for epoch in range(start_from, training_epochs):\n",
    "    print('Start Epoch:', epoch)\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n",
    "        writer.add_summary(s, global_step=global_step)\n",
    "        global_step += 1\n",
    "\n",
    "        avg_cost += sess.run(cost, feed_dict=feed_dict) / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Saving network...\")\n",
    "    sess.run(last_epoch.assign(epoch + 1))\n",
    "    if not os.path.exists(CHECK_POINT_DIR):\n",
    "        os.makedirs(CHECK_POINT_DIR)\n",
    "    saver.save(sess, CHECK_POINT_DIR + \"/model\", global_step=i)\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "'''\n",
    "\n",
    "...\n",
    "\n",
    "Successfully loaded: ./tb/mnist/model-549\n",
    "Start learning from: 2\n",
    "Epoch: 2\n",
    "\n",
    "...\n",
    "tensorboard --logdir tb/\n",
    "Starting TensorBoard b'41' on port 6006\n",
    "(You can navigate to http://10.0.1.4:6006)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.982\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18b01860518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# # Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Datasets' object has no attribute 'train_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-579df4bf29f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_train_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-579df4bf29f4>\u001b[0m in \u001b[0;36mget_train_batch\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_train_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mchoice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mbatch_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Datasets' object has no attribute 'train_x'"
     ]
    }
   ],
   "source": [
    "#ex2\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "#par\n",
    "learning_rate=0.001\n",
    "training_epochs=15\n",
    "batch_size=128\n",
    "SUMMARY_DIR = './mnist'\n",
    "\n",
    "#load\n",
    "MNIST = input_data.read_data_sets(\"./MNIST_data\",one_hot=True)\n",
    "\n",
    "with tf.name_scope('input') as scope:\n",
    "    X = tf.placeholder(tf.float32, [None,784],name='image')\n",
    "    y = tf.placeholder(tf.float32,[None,10],name = 'label')\n",
    "\n",
    "with tf.variable_scope('layer1') as scope :\n",
    "    W1 = tf.get_variable(\"W\",shape=[784,512],initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([512]))\n",
    "    L1 = tf.nn.relu(tf.add(tf.matmul(X,W1),b1))\n",
    "    \n",
    "    tf.summary.histogram(\"X\",X)\n",
    "    tf.summary.histogram(\"weights\",W1)\n",
    "    tf.summary.histogram(\"bias\",b1)\n",
    "    tf.summary.histogram(\"layer\",L1)\n",
    "    \n",
    "with tf.variable_scope('layer2') as scope:\n",
    "    W2 = tf.get_variable(\"W\",shape=[512,10],initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([10]))\n",
    "    y_=tf.add(tf.matmul(L1,W2),b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weights\",W2)\n",
    "    tf.summary.histogram(\"bias\",b2)\n",
    "    tf.summary.histogram(\"logits\",y_)\n",
    "\n",
    "\n",
    "def get_train_batch(batch_size,fake_data =False):\n",
    "    if fake_data:\n",
    "        fake_image = [1] * 784\n",
    "        if self.one_hot:\n",
    "\n",
    "            fake_label = [1] + [0] * 9\n",
    "\n",
    "        else:\n",
    "            fake_label = 0\n",
    "        return [fake_image for _ in xrange(batch_size)], [fake_label for _ in xrange(batch_size)]\n",
    "    choice = np.random.randint(low = 0, high = MNIST.train.num_examples)\n",
    "    start = choice\n",
    "    e = start + batch_size\n",
    "    if(e> MNIST.train_num_examples) :\n",
    "        end = MNIST.train_num-examples -1\n",
    "    else :\n",
    "        end = e\n",
    "    return MNIST.train._images[start:end], MNIST.train._labels[start:end]\n",
    "    \n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_,labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "summary=tf.summary.merge_all()\n",
    "global_step =0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR,sess.graph)\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch=int(MNIST.train.num_examples/batch_size)\n",
    "        avg_loss=0\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs,batch_ys = get_train_batch(batch_size)\n",
    "            feed_dict = {X: batch_xs, y: batch_ys}\n",
    "            s,l, _ = sess.run([summary,loss,optimizer],feed_dict=feed_dict)\n",
    "            writer.add_summary(s,global_step=global_step)\n",
    "            global_step+=1\n",
    "            avg_loss +=l\n",
    "        print('Epoch:','%02d'%(epoch+1),'loss=','{:.6f}'.format(avg_loss/total_batch0))\n",
    "        \n",
    "    correction_prediction = tf.equal(tf.argmax(y_,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    acc = sess.run(accuracy,feed_dict={X: MNIST.test.images,y:MNIST.test.labels})\n",
    "    print('Test accuracy:',acc)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
