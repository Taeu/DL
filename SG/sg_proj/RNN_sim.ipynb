{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "sentence = (\"\"\"That, poor contempt, or claim'd thou slept so faithful, I may contrive our father; and, in their defeated queen, Her flesh broke me and puttance of expedition house, And in that same that ever I lament this stomach, And he, nor Butly and my fury, knowing everything Grew daily ever, his great strength and thought The bright buds of mine own.\n",
    "\n",
    "BIONDELLO:\n",
    "Marry, that it may not pray their patience.'\n",
    "\n",
    "KING LEAR:\n",
    "The instant common maid, as we may less be a brave gentleman and joiner: he that finds us with wax And owe so full of presence and our fooder at our staves. It is remorsed the bridal's man his grace for every business in my tongue, but I was thinking that he contends, he hath respected thee.\n",
    "\n",
    "BIRON:\n",
    "She left thee on, I'll die to blessed and most reasonable Nature in this honour, and her bosom is safe, some others from his speedy-birth, a bill and as Forestem with Richard in your heart Be question'd on, nor that I was enough: Which of a partier forth the obsers d'punish'd the hate To my restraints would not then be got as I partly.\n",
    "\n",
    "AUTOLYCUS:\n",
    "Hath sat her love within this man, that was foul prayers Which are much thus from them with thee; I am not ever thought To make that with a wise exclaim, as I am sure; To say well like a dotage on the fixed cease, And let mine eyes may straight sole sword conveyard, That dust-confounded by a land to their command Then puissant with a grief's: it should be so and dead, Till he shall fail his sister; and in true and good, To see me for the other, hath not heard a midwife Loud from my service and thy sweetly daughter got The single strange words pent is all his steed: Stay from us in, as he hath we brought me into the Milthiness.\"\"\"         \n",
    "               )\n",
    "char_set = list(set(sentence))\n",
    "\n",
    "char_dic= {w: i for i,w in enumerate(char_set)}\n",
    "\n",
    "data_dim = len(char_set)\n",
    "hidden_size = len(char_set)\n",
    "num_classes = len(char_set)\n",
    "seq_length = 30 # any number #변경\n",
    "num_layer = 5\n",
    "\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(0,len(sentence)-seq_length):\n",
    "    x_str = sentence[i:i+seq_length]\n",
    "    y_str = sentence[i+1:i+seq_length+1]\n",
    "    #print(i, x_str, '-->', y_str)\n",
    "    \n",
    "    x= [char_dic[c] for c in x_str]\n",
    "    y = [char_dic[c] for c in y_str]\n",
    "    \n",
    "    dataX.append(x)\n",
    "    dataY.append(y)\n",
    "    \n",
    "batch_size = len(dataX)\n",
    "\n",
    "X = tf.placeholder(tf.int32,[None, seq_length])\n",
    "Y = tf.placeholder(tf.int32,[None,seq_length])\n",
    "\n",
    "X_one_hot = tf.one_hot(X,num_classes)\n",
    "\n",
    "cell = rnn.BasicLSTMCell(hidden_size)\n",
    "#cell  = tf.rnn.GRUCell(num_units=hidden_size)\n",
    "cell = rnn.MultiRNNCell([cell]*num_layer, state_is_tuple= True)\n",
    "# layer 수 변경\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell,X_one_hot,dtype = tf.float32)\n",
    "X_for_softmax = tf.reshape(outputs,[-1,hidden_size])\n",
    "softmax_w = tf.get_variable(\"softmax_w\",[hidden_size,num_classes])\n",
    "softmax_b = tf.get_variable(\"softmax_b\",[num_classes])\n",
    "outputs = tf.matmul(X_for_softmax,softmax_w)+softmax_b\n",
    "\n",
    "#reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs,[batch_size,seq_length,num_classes])\n",
    "#all weigths are 1 (equal weights)\n",
    "weights =tf.ones([batch_size,seq_length])\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs,targets=Y,weights=weights)\n",
    "mean_loss = tf.reduce_mean(sequence_loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = 0.1).minimize(mean_loss)\n",
    "\n",
    "sess =tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(500):\n",
    "    _, l, results = sess.run([train_op,mean_loss,outputs],feed_dict={ X:dataX, Y:dataY})\n",
    "    for j , result in enumerate(results):\n",
    "        index = np.argmax(result,axis=1)\n",
    "        #print(i, j, ''.join([char_set[t] for t in index]), l)\n",
    "\n",
    "results = sess.run(outputs,feed_dict={X:dataX})\n",
    "sent =\"\"\n",
    "for j, result in enumerate(results):\n",
    "    index = np.argmax(result,axis=1)\n",
    "    if j is 0:\n",
    "        sent=sent+\"\".join([char_set[t] for t in index])\n",
    "    else :\n",
    "        sent=sent+char_set[index[-1]]\n",
    "        \n",
    "accuracy = sum([sent[x]==sentence[x+1] for x in range(len(sent))])/len(sent)\n",
    "print('seq_length = ',seq_length,', # of rnn layer = ',num_layer,', accuracy = ', accuracy)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
